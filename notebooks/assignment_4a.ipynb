{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ_pmgxvGur9"
   },
   "source": [
    "# Assignment 4a - Variational Auto-Encoders\n",
    "## Deep Learning Course - Vrije Universiteit Amsterdam, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEneMITS2agU"
   },
   "source": [
    "#### Instructions on how to use this notebook:\n",
    "\n",
    "This notebook is hosted on Google Colab. To be able to work on it, you have to create your own copy. Go to *File* and select *Save a copy in Drive*.\n",
    "\n",
    "You can also avoid using Colab entirely, and download the notebook to run it on your own machine. If you choose this, go to *File* and select *Download .ipynb*.\n",
    "\n",
    "The advantage of using Colab is that you can use a GPU. You can complete this assignment with a CPU, but it will take a bit longer. Furthermore, we encourage you to train using the GPU not only for faster training, but also to get experience with this setting. This includes moving models and tensors to the GPU and back. This experience is very valuable because for various models and large datasets (like large CNNs for ImageNet, or Transformer models trained on Wikipedia), training on GPU is the only feasible way.\n",
    "\n",
    "The default Colab runtime does not have a GPU. To change this, go to *Runtime - Change runtime type*, and select *GPU* as the hardware accelerator. The GPU that you get changes according to what resources are available at the time, and its memory can go from a 5GB, to around 18GB if you are lucky. If you are curious, you can run the following in a code cell to check:\n",
    "\n",
    "```sh\n",
    "!nvidia-smi\n",
    "```\n",
    "\n",
    "Note that despite the name, Google Colab does  not support collaborative work without issues. When two or more people edit the notebook concurrently, only one version will be saved. You can choose to do group programming with one person sharing the screen with the others, or make multiple copies of the notebook to work concurrently.\n",
    "\n",
    "**Submission:** Upload your notebook in .ipynb format to Canvas. The code and answers to the questions in the notebook are sufficient, no separate report is expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBgoJIpdLI2Y",
    "outputId": "af4a84d0-6d6d-4d9d-86d4-02cc147edce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 21 18:48:00 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   67C    P0    28W /  70W |    902MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsdc7fDp40rQ"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment, we are going to implement a Variational Auto-Encoder (VAE). A VAE is a likelihood-based deep generative model that consists of a stochastic encoder (a variational posterior over latent variables), a stochastic decoder, and a marginal distribution over latent variables (a.k.a. a prior). The model was originally proposed in two concurrent papers:\n",
    "- [Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.](https://arxiv.org/abs/1312.6114)\n",
    "- [Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. \"Stochastic backpropagation and approximate inference in deep generative models.\" International conference on machine learning. PMLR, 2014.](https://proceedings.mlr.press/v32/rezende14.html)\n",
    "\n",
    "You can read more about VAEs in Chapter 4 of the following book:\n",
    "- [Tomczak, J.M., \"Deep Generative Modeling\", Springer, 2022](https://link.springer.com/book/10.1007/978-3-030-93158-2)\n",
    "\n",
    "In particular, the goals of this assignment are the following:\n",
    "\n",
    "- Understand how VAEs are formulated\n",
    "- Implement components of VAEs using PyTorch\n",
    "- Train and evaluate a model for image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvsuVNczG6pP"
   },
   "source": [
    "### Theory behind VAEs\n",
    "\n",
    "VAEs are latent variable models trained with variational inference. In general, the latent variable models define the following generative process:\n",
    "\\begin{align}\n",
    "1.\\ & \\mathbf{z} \\sim p_{\\lambda}(\\mathbf{z}) \\\\\n",
    "2.\\ & \\mathbf{x} \\sim p_{\\theta}(\\mathbf{x}|\\mathbf{z})\n",
    "\\end{align}\n",
    "\n",
    "In plain words, we assume that for observable data $\\mathbf{x}$, there are some latent (hidden) factors $\\mathbf{z}$. Then, the training objective is log-likelihood function of the following form:\n",
    "$$\n",
    "\\log p_{\\vartheta}(\\mathbf{x})=\\log \\int p_\\theta(\\mathbf{x} \\mid \\mathbf{z}) p_\\lambda(\\mathbf{z}) \\mathrm{d} \\mathbf{z} .\n",
    "$$\n",
    "\n",
    "The problem here is the intractability of the integral if the dependencies between random variables $\\mathbf{x}$ and $\\mathbf{z}$ are non-linear and/or the distributions are non-Gaussian.\n",
    "\n",
    "By introducing variational posteriors $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$, we get the following lower bound (the Evidence Lower Bound, ELBO):\n",
    "$$\n",
    "\\log p_{\\vartheta}(\\mathbf{x}) \\geq \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_\\theta(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\mathrm{KL}\\left(q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\| p_\\lambda(\\mathbf{z})\\right) .\n",
    "$$\n",
    "\n",
    "Note that we want to *maximize* this objective, therefore, in the code you are going to have to implement NELBO (negative ELBO) as a loss function (i.e., a minimization task). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suzhlbWqxtD9"
   },
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjxkigYLxpB7"
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE!\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cm23hRm6CqGh",
    "outputId": "498324b5-5251-45e6-c768-f661d62fafd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available device is cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and determine the device\n",
    "if torch.cuda.is_available():\n",
    "  device = 'cuda'\n",
    "else:\n",
    "  device = 'cpu'\n",
    "\n",
    "print(f'The available device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81CxONpmMulC",
    "outputId": "d6004962-94a9-4a2b-fd9a-f64c90a26cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# mount drive: WE NEED IT FOR SAVING IMAGES!\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoPb92zNM4UY"
   },
   "outputs": [],
   "source": [
    "# PLEASE CHANGE IT TO YOUR OWN GOOGLE DRIVE!\n",
    "images_dir = '/content/gdrive/My Drive/Colab Notebooks/Results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3zs31tOyCmq"
   },
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF0agzL7tDHK"
   },
   "source": [
    "Let us define some useful log-distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIBNVRNJtHSd"
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE\n",
    "PI = torch.from_numpy(np.asarray(np.pi))\n",
    "EPS = 1.e-5\n",
    "\n",
    "\n",
    "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "\n",
    "def log_bernoulli(x, p, reduction=None, dim=None):\n",
    "    pp = torch.clamp(p, EPS, 1. - EPS)\n",
    "    log_p = x * torch.log(pp) + (1. - x) * torch.log(1. - pp)\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "\n",
    "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "\n",
    "def log_standard_normal(x, reduction=None, dim=None):\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2. * PI) - 0.5 * x**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2LLOs0kn7iw"
   },
   "source": [
    "## Implementing VAEs\n",
    "\n",
    "The goal of this assignment is to implement four classes:\n",
    "- `Encoder`: this class implements the encoder (variational posterior), $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$.\n",
    "- `Decoder`: this class implements the decoded (the conditional likelihood), $p_{\\theta}(\\mathbf{x}|\\mathbf{z})$.\n",
    "- `Prior`: this class implements the marginal over latents (the prior), $p_{\\lambda}(\\mathbf{z})$.\n",
    "- `VAE`: this class combines all components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnziXXLPJhp6"
   },
   "source": [
    "#### Question 0: (3 pt) \n",
    "**Fully-connected Neural Networks (MLPs) or Convolutional Neural Networks**\n",
    "\n",
    "This is not a real question but rather a comment. You are asked to implement your VAE using fully connected neural networks (MLPs) or convolutional neural networks (ConvNets). \n",
    "\n",
    "There is a difference in grading of this assignment based on your decision:\n",
    "- **If you decide to implement your VAE with MLPs and the model works properly, you get 1 pt.**\n",
    "- **If you decide to implement your VAE with ConvNets and the model works properly, you get 3 pts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cXhOwKAzW6Z"
   },
   "source": [
    "### Encoder\n",
    "We start with `Encoder`. Please remember that we assume the Gaussian variational posterior with a diagonal covariance matrix.\n",
    "\n",
    "Feel free to add other methods to the class as well as arguments to the class initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrwQXSuEoFfH"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# NOTE: The class must containt the following functions: \n",
    "# (i) reparameterization\n",
    "# (ii) sample\n",
    "# Moreover, forward must return the log-probability of variational posterior for given x, i.e., log q(z|x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        pass\n",
    "\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        \"\"\"\n",
    "        Sample from the encoder. \n",
    "        If x is given (not equal to None), then copmute variational posterior distribution q(z|x) and sample from it.\n",
    "        Otherwise, use `mu_e` and `log_var_e` as parameter of the variational posterior and sample from it.\n",
    "\n",
    "        x: torch.tensor, with dimensionality (mini-batch, x_dim)\n",
    "             a mini-batch of data points\n",
    "        mu_e: torch.tensor, with dimensionality (mini-batch, x_dim)\n",
    "             mean vector of the variational posterior\n",
    "        log_var_e: torch.tensor, with dimensionality (mini-batch, x_dim)\n",
    "             log variance of the variational posterior\n",
    "        return: z: torch.tensor, with dimensionality (mini-batch, z_dim)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute log-probability of variational posterior for given x, i.e., log q(z|x)\n",
    "        x: torch.tensor, with dimensionality (mini-batch, x_dim)\n",
    "             a mini-batch of data points\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XVlH5OUzdgJ"
   },
   "source": [
    "Please answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1BNAH02zjjt"
   },
   "source": [
    "\n",
    "#### Question 1 (0.5 pt)\n",
    "\n",
    "Please explain the reparameterization trick and provide a mathematical formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlxYq7-gzo_o"
   },
   "source": [
    "ANSWER:\n",
    "\n",
    "VAEs learn to predict the mean $\\mu$ and the standard deviation $\\sigma$ of a Gaussian distribution to generate sample $z$ from a parameterized distribution. The decoder then decodes sample $z$. However, this sampling operation is not differentiable. Additionally, if we use sample $z$ from $q\\phi(z|x)$ for the ELBO and calculate gradients to update the parameters of a neural network, there may be a large variance of the gradient. The reparameterization trick solves this. The essence of this trick is to move the stochasticity to independent random variables by reparameterizing the Gaussian distribution. The predictions are separated from the stochastic sampling element.\n",
    "\n",
    "For a Gaussian distribution, the noise is considered a standard normal distribution: as a Gaussian with mean 0 and variance 1. Given Gaussian random variable $z$ with mean $\\mu$ and variance $\\sigma^2$, and independent random variable $\\epsilon \\sim 𝒩(\\epsilon | 0,1)$, then the following holds:\n",
    "\n",
    "$z = \\mu + \\sigma \\cdotp \\epsilon$.\n",
    "\n",
    "Here, we use a fixed source of noise $\\epsilon$ where we can sample from. When sampling $\\epsilon$ from the standard Gaussian and applying the transformation from above, we obtain a sample from $𝒩 (z|\\mu, \\sigma)$.\n",
    "\n",
    "The noise term is no longer parameterized by the model. In this way, the gradient is calculated with respect to a deterministic function instead of a stochastic element. The gradient path goes through a non-stochastic node. Hence, the sampling operation is made differentiable without large variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpRgXdtBzt3-"
   },
   "source": [
    "#### Question 2 (0.25 pt)\n",
    "\n",
    "Please write down mathematically the log-probability of the encoder (variational posterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET-mMAg10Ewv"
   },
   "source": [
    "ANSWER:\n",
    "\n",
    "The log-probability of the encoder/variational posterior is:\n",
    "\n",
    "$\\log q_\\phi(z|x)$ \n",
    "\n",
    "For the variational posterior, $z$ is sampled from a multivariate Gaussian with mean $\\mu_\\phi$ and a diagonal covariance matrix $\\sigma_\\phi^2$:\n",
    "\n",
    "$q_\\phi(z|x) = 𝒩(z|\\mu_\\phi(x), \\mathrm{diag}[\\sigma^2_\\phi(x)])$\n",
    "\n",
    "where $\\mu_\\phi(x)$ and $\\sigma^2_\\phi(x)$ are given by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhNTy5mn0XDT"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder is the conditional likelihood, i.e., $p(\\mathbf{x}|\\mathbf{z})$. Please remember that we must decide on the form of the distribution (e.g., Bernoulli, Gaussian, Categorical). Please discuss it with a TA or a lecturer if you are in doubt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vTmKHwrpUVa"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# NOTE: The class must containt the following function: \n",
    "# (i) sample\n",
    "# Moreover, forward must return the log-probability of the conditional likelihood function for given z, i.e., log p(x|z)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "    def sample(self, z):\n",
    "        \"\"\"\n",
    "        For a given latent code compute parameters of the conditional likelihood \n",
    "        and sample x ~ p(x|z)\n",
    "\n",
    "        z: torch.tensor, with dimensionality (mini-batch, z_dim)\n",
    "\n",
    "        return:\n",
    "        x: torch.tensor, with dimensionality (mini-batch, x_dim)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        \"\"\"\n",
    "        Compute the log probability: log p(x|z). \n",
    "        z: torch.tensor, with dimensionality (mini-batch, z_dim)\n",
    "        x: torch.tensor, with dimensionality (mini-batch, x_dim)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xjbNkNL01DP"
   },
   "source": [
    "Please answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjDvPaBj04cA"
   },
   "source": [
    "#### Question 3 (0.5 pt)\n",
    "\n",
    "Please explain your choice of the distribution for image data used in this assignment. Additionally, please write it down mathematically (if you think that presenting it as the log-probability, then please do it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLZEzmGI1Ok-"
   },
   "source": [
    "ANSWER:\n",
    "\n",
    "The MNIST dataset contains 8-bit encoded images, where every pixel has a discrete value from $0$ to $255$. To put this in mathematical notation, an image is represented as $x$ with $x= \\mathcal{X}^D \\in \\{0, 1, \\ldots, 255\\}$, $D$ being the dimensionality of the image.\n",
    "\n",
    "Since we are dealing with discrete variables, it makes sense to model the probabilities of all discrete pixel values per pixel with a $\\textbf{categorical}$, or formally a multinomial probability distribution. This multinomial distribution $\\theta(\\mathbf{z})$ depends on the $\\mathbf{z}$ we sampled (see answer to Question 4 for the method of sampling).\n",
    "\n",
    "How we go from $\\mathbf{z})$ to probability values for all pixel values is parameterized by a neural network with a softmax at the end: $\\theta(\\mathbf{z}) =$ softmax(NN($\\mathbf{z}$)).\n",
    "\n",
    "Given $\\theta(\\mathbf{z})$, we can compute the log likelihood of $\\mathbf{x}$: $p_{\\theta}(\\mathbf{x}|z) $. For a multi-dimensional image vector $\\mathbf{x}$, this entails summing the log probability of all true pixel values in the original image.\n",
    "\n",
    "$\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}) = \\sum^D_d \\log p_\\theta(x_{d,\\text{true}}|\\mathbf{z}) $\n",
    "This can be seen in the $\\texttt{log_categorical()}$ function.\n",
    "\n",
    "Alternatively, if the in put of every pixel is binary (black-white) such that $x= \\mathcal{X}^D \\in \\{0, 1\\}$, the discrete probability values for all pixel values $x_d$ can be given by a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhbWamId1eGt"
   },
   "source": [
    "#### Question 4 (0.5 pt)\n",
    "\n",
    "Please explain how one can sample from the distribution chosen by you. Please be specific and formal (i.e., provide mathematical formulae). If applicable, please provide a code snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:\n",
    "\n",
    "The encoder network outputs $M$ values for the mean $\\mu_{\\phi}$ and $M$ values for the log variance $\\log \\sigma^2_{\\phi}$. The variance is in log form so that negative and zero values outputted by the network can still be converted to valid values for $\\sigma^2$ by taking $\\exp(\\log \\sigma^2)$. The means and variances are represented by vectors $\\boldsymbol{\\mu}*{\\theta}$, $\\boldsymbol{\\sigma^2}*{\\theta} \\in \\mathbb{R}^{M}.$\n",
    "\n",
    "We now sample $\\mathbf{z} \\sim  q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ using the reparametrization trick:\n",
    "$\\mathbf{z} = \\boldsymbol{\\mu}*{\\theta} + \\boldsymbol{\\sigma^2}*{\\theta} \\odot \\boldsymbol{\\epsilon}$, $\\odot$ denoting elementwise multiplication. $\\boldsymbol{\\epsilon}$ is sampled from a standard multivariate Gaussian: $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})$.\n",
    "\n",
    "Sampling $\\mathbf{z}$ in this manner returns $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{z} |\\boldsymbol{\\mu}_{\\theta}, \\boldsymbol{\\sigma^2}\\mathbf{I})$.\n",
    "\n",
    "To go from our sampled $\\mathbf{z}$ to generated image, we feed $\\mathbf{z}$ to the decoder net, obtaining probabilities for all discrete pixel values $\\{0, 1, 2, \\ldots, 255\\}$ for all pixels. Using these probabilities, for every pixel the corresponding multinomial distribution is sampled to obtain a pixel value. The sampled values can then be compiled in a generated $\\hat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLIEwIiw00op"
   },
   "source": [
    "### Prior\n",
    "\n",
    "The prior is the marginal distribution over latent variables, i.e., $p(\\mathbf{z})$. It plays a crucial role in the generative process and also in synthesizing images of a better quality.\n",
    "\n",
    "In this assignment, you are asked to implement a prior that is learnable (e.g., parameterized by neural networks). If you decide to implement the standard Gaussian prior only, then please be aware that you will not get any points.\n",
    "\n",
    "\n",
    "For the learnable prior you can choose one of the following options:\n",
    "\n",
    "\n",
    "*   Mixture of Gaussians\n",
    "*   Normalizing Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQIvee5Cp69V"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# NOTES:\n",
    "# (i) Implementing the standard Gaussian prior does not give you any points!\n",
    "# (ii) The function \"sample\" must be implemented.\n",
    "# (iii) The function \"forward\" must return the log-probability, i.e., log p(z)\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(Prior, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        pass\n",
    "\n",
    "    def forward(self, z):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0KH9f2O_PDg"
   },
   "source": [
    "#### Question 5 (2 pts max)\n",
    "\n",
    "**Option 1 (0 pt):  Standard Gaussian**\n",
    "\n",
    "**NOTE: *If you decide to use the standard Gaussian prior, please indicate it in your answer. However, you will get 0 pt for this question.***\n",
    "\n",
    "**Option 2 (0.5 pt): Mixture of Gaussains**\n",
    "\n",
    "Please do the following:\n",
    "- (0.25 pt) Please explain your prior and write it down mathematically\n",
    "- (0.15 pt) Please write down its sampling procedure (if necessary, please add a code snippet).\n",
    "- (0.1 pt) Please write down its log-probability (a mathematical formula).\n",
    "\n",
    "**Option 3 (2 pts): Normalizing Flow**\n",
    "\n",
    "Please do the following:\n",
    "- (1 pt) Please explain your prior and write it down mathematically\n",
    "- (0.5 pt) Please write down its sampling procedure (if necessary, please add a code snippet).\n",
    "- (0.5 pt) Please write down its log-probability (a mathematical formula)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOM4QM9I_62d"
   },
   "source": [
    "### Complete VAE\n",
    "\n",
    "The last class is `VAE` tha combines all components. Please remember that this class must implement the **Negative ELBO** in `forward`, as well as `sample` (*hint*: it is a composition of `sample` functions from the prior and the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQpf-BeSqA6V"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# This class combines Encoder, Decoder and Prior.\n",
    "# NOTES:\n",
    "# (i) The function \"sample\" must be implemented.\n",
    "# (ii) The function \"forward\" must return the negative ELBO. Please remember to add an argument \"reduction\" that is either \"mean\" or \"sum\".\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, reduction='mean'):\n",
    "        # Please correctly implement this function\n",
    "        \n",
    "        # Negative ELBO (NELBO)\n",
    "        NELBO = 0.\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return NELBO.sum()\n",
    "        else:\n",
    "            return NELBO.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaJgXPYyAmeJ"
   },
   "source": [
    "#### Question 6 (0.5 pt)\n",
    "\n",
    "Please write down mathematically the **Negative ELBO** and provide a code snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THYyO-G7AkSQ"
   },
   "source": [
    "ANSWER: [Please fill in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLhgze7DA4yx"
   },
   "source": [
    "### Evaluation and training functions\n",
    "\n",
    "**Please do not remove or modify them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9Dr3a6lqJ0W"
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE\n",
    "\n",
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, (test_batch, _) in enumerate(test_loader):\n",
    "        test_batch = test_batch.to(device)\n",
    "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def samples_real(name, test_loader, shape=(28,28)):\n",
    "    # real images-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x, _ = next(iter(test_loader))\n",
    "    x = x.to('cpu').detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], shape)\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def samples_generated(name, data_loader, shape=(28,28), extra_name=''):\n",
    "    x, _ = next(iter(data_loader))\n",
    "    x = x.to('cpu').detach().numpy()\n",
    "\n",
    "    # generations-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = model_best.sample(num_x * num_y)\n",
    "    x = x.to('cpu').detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], shape)\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_curve(name, nll_val):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ABgMeG0qFAP"
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE\n",
    "\n",
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader, shape=(28,28)):\n",
    "    nll_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, (batch, _) in enumerate(training_loader):\n",
    "            batch = batch.to(device)\n",
    "            loss = model.forward(batch, reduction='mean')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "        else:\n",
    "            if loss_val < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_val\n",
    "                patience = 0\n",
    "\n",
    "                samples_generated(name, val_loader, shape=shape, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "\n",
    "    return nll_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWr8N2u2qNTu"
   },
   "source": [
    "### Setup\n",
    "\n",
    "**NOTE: *Please comment your code! Especially if you introduce any new variables (e.g., hyperparameters).***\n",
    "\n",
    "In the following cells, we define `transforms` for the dataset. Next, we initialize the data, a directory for results and some fixed hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFTE5jtYpxDV"
   },
   "outputs": [],
   "source": [
    "# PLEASE DEFINE APPROPRIATE TRANFORMS FOR THE DATASET\n",
    "transforms_train = None\n",
    "\n",
    "transforms_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SDcOBbGCM8z"
   },
   "source": [
    "Please do not modify the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXHitzrYqNhY"
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE\n",
    "#-dataset\n",
    "dataset = MNIST('/files/', train=True, download=True,\n",
    "                      transform=transforms_train\n",
    "                )\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [50000, 10000], generator=torch.Generator().manual_seed(14))\n",
    "\n",
    "test_dataset = MNIST('/files/', train=False, download=True,\n",
    "                      transform=transforms_test\n",
    "                     )\n",
    "#-dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#-creating a dir for saving results\n",
    "name = 'vae'\n",
    "result_dir = images_dir + 'results/' + name + '/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "\n",
    "#-hyperparams (please do not modify them for the final report)\n",
    "num_epochs = 1000 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmKDXMI0B231"
   },
   "source": [
    "In the next cell, please initialize the model. Please remember about commenting your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b73aaBDxqSYb"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE COMES HERE:\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC8AkWt4CURT"
   },
   "source": [
    "Please initialize the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3nTSDe7ql08"
   },
   "outputs": [],
   "source": [
    "# PLEASE DEFINE YOUR OPTIMIZER\n",
    "lr = 0.002\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79odxtRjCaix"
   },
   "source": [
    "#### Question 7 (0.5 pt)\n",
    "\n",
    "Please explain the choice of the optimizer, and comment on the choice of the hyperparameters (e.g., the learing reate value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEjOlYN9Ft_B"
   },
   "source": [
    "ANSWER:\n",
    "\n",
    "As an optimizer, we chose pytorch’s implementation of Adamax (Kingma and Ba, 2014). Adam (Adaptive Moment Estimation) is an optimizer that adaptively computes individual learning rates for each parameter. Adamax is a variant of Adam that is based on the infinity norm, aimed to accelarate the optimization process. Based on recommendations by Sebastian Ruder (Ruder, 2016), we used Adamax with the hyperparameters 0.002,  $η=0.002$, $β_1=0.9$, and $β_2=0.999$ and $\\epsilon = 1e^{-8}$.\n",
    "<br><br>\n",
    "\n",
    "Kingma, Diederik P., and Jimmy Ba. \"Adam: A method for stochastic optimization.\" *arXiv preprint arXiv:1412.6980* (2014).\n",
    "\n",
    "Ruder, Sebastian. \"An overview of gradient descent optimization algorithms.\" *arXiv preprint arXiv:1609.04747* (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5GrzUcHFweG"
   },
   "source": [
    "### Training and final evaluation\n",
    "\n",
    "In the following two cells, we run the training and the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VD7WuY6bqnBK"
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY\n",
    "# Training procedure\n",
    "nll_val = training(name=result_dir + name, max_patience=max_patience, \n",
    "                   num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                   training_loader=train_loader, val_loader=val_loader,\n",
    "                   shape=(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAuMt9_wquOI"
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY\n",
    "# Final evaluation\n",
    "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "samples_real(result_dir + name, test_loader)\n",
    "samples_generated(result_dir + name, test_loader, extra_name='_FINAL')\n",
    "\n",
    "plot_curve(result_dir + name, nll_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaBwGtSJF8ag"
   },
   "source": [
    "### Results and discussion\n",
    "\n",
    "After a successful training of your model, we would like to ask you to present your data and analyze it. Please answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4WZkoiHFyZm"
   },
   "source": [
    "\n",
    "#### Question 8 (1 pt)\n",
    "\n",
    "Please select the real data, and the final generated data and include them in this report. Please comment on the following:\n",
    "- (0.5 pt) Do you think the model was trained properly by looking at the generations? Please motivate your answer well.\n",
    "- (0.5 pt) What are potential problems with evaluating a generative model by looking at generated data? How can we evalute generative models (ELBO or NLL do not count as an answer)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8yQ2T9GIuvc"
   },
   "source": [
    "ANSWER:\n",
    "\n",
    "Most of the generations produced by our model contain a discernable number and resemble the MNIST images used as input. Therefore, we believe the model was trained properly. Previous iterations where the latent space was smaller produced unrecognizable data with white spots at seemingly random positions.\n",
    "\n",
    "Potential problems with evaluating a generative model by looking at generated data are that since this is a qualitative way of assessing the results, it is subjective and affected by biases (Borji, 2019). It’s also dependent on the knowledge of the evaluator about what is realistic to expect. Since someone else may have a different opinion about the model’s success, this way of evaluation is not reproducible. Another problem is that there are a lot of images for which it would take a lot of time to look at them all.\n",
    "\n",
    "Two alternative ways of evaluating generative models are (1) nearest neighbor analysis or (2) classification by a pre-trained network. The Nearest Neighbors approach involves selection of real images that are in the same domain or of the same object. For our MNIST dataset, this would be images of the same number. These images are used for comparison. The difference or similarity between the images can be calculated with distance measures like Euclidean distance between pixel data. Another method could be to use a pre-trained neural network to classify the images generated by the generative model (Salimans et al., 2016). For each generated image, the classification model assigns the probability of belonging to each class. Summarizing these probabilities gives a score that can be used for evaluation.\n",
    "\n",
    "<br><br>\n",
    "Borji, Ali. \"Pros and cons of gan evaluation measures.\" *Computer Vision and Image Understanding* 179 (2019): 41-65.\n",
    "\n",
    "Salimans, Tim, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. \"Improved techniques for training gans.\" *Advances in neural information processing systems* 29 (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmyH318fIwc9"
   },
   "source": [
    "#### Question 9 (1.25 pt)\n",
    "\n",
    "Please include the plot of the negative ELBO. Please comment on the following:\n",
    "- (0.25 pt) Is the training of your VAE stable or unstable? Why?\n",
    "- (1 pt) What is the influence of the optimizer on your model? Do the hyperparameter values of the optimizer important and how do they influence the training? Motivate well your answer (e.g., run the script with more than one learning rate and present two plots here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-10GAVZtKTj2"
   },
   "source": [
    "ANSWER: [Please fill in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7QhyAMms8-L"
   },
   "source": [
    "# Grading (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juIdxXhos-mV"
   },
   "source": [
    "- Question 0: 3pt\n",
    "- Question 1: 0.5pt \n",
    "- Question 2: 0.25pt \n",
    "- Question 3: 0.5pt \n",
    "- Question 4: 0.5pt \n",
    "- Question 5: 2pt \n",
    "- Question 6: 0.5pt\n",
    "- Question 7: 0.5pt\n",
    "- Question 8: 1pt\n",
    "- Question 9: 1.25pt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
